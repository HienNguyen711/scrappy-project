{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from urllib import robotparser\n",
    "from urllib.parse import urljoin\n",
    "from urllib.error import URLError, HTTPError, ContentTooShortError\n",
    "from lxml.html import fromstring\n",
    "from chp1.throttle import Throttle\n",
    "\n",
    "\n",
    "def download(url, user_agent='wswp', num_retries=2, charset='utf-8', proxy=None):\n",
    "    \"\"\" Download a given URL and return the page content\n",
    "        args:\n",
    "            url (str): URL\n",
    "        kwargs:\n",
    "            user_agent (str): user agent (default: wswp)\n",
    "            charset (str): charset if website does not include one in headers\n",
    "            proxy (str): proxy url, ex 'http://IP' (default: None)\n",
    "            num_retries (int): number of retries if a 5xx error is seen (default: 2)\n",
    "    \"\"\"\n",
    "    print('Downloading:', url)\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header('User-agent', user_agent)\n",
    "    try:\n",
    "        if proxy:\n",
    "            proxy_support = urllib.request.ProxyHandler({'http': proxy})\n",
    "            opener = urllib.request.build_opener(proxy_support)\n",
    "            urllib.request.install_opener(opener)\n",
    "        resp = urllib.request.urlopen(request)\n",
    "        cs = resp.headers.get_content_charset()\n",
    "        if not cs:\n",
    "            cs = charset\n",
    "        html = resp.read().decode(cs)\n",
    "    except (URLError, HTTPError, ContentTooShortError) as e:\n",
    "        print('Download error:', e)\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "                # recursively retry 5xx HTTP errors\n",
    "                return download(url, num_retries - 1)\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_robots_parser(robots_url):\n",
    "    \" Return the robots parser object using the robots_url \"\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(robots_url)\n",
    "    rp.read()\n",
    "    return rp\n",
    "\n",
    "\n",
    "def get_links(html):\n",
    "    \" Return a list of links (using simple regex matching) from the html content \"\n",
    "    # a regular expression to extract all links from the webpage\n",
    "    webpage_regex = re.compile(\"\"\"<a[^>]+href=[\"'](.*?)[\"']\"\"\", re.IGNORECASE)\n",
    "    # list of all links from the webpage\n",
    "    return webpage_regex.findall(html)\n",
    "\n",
    "\n",
    "def scrape_callback(url, html):\n",
    "    \"\"\" Scrape each row from the country data using XPath and lxml \"\"\"\n",
    "    fields = ('area', 'population', 'iso', 'country', 'capital',\n",
    "              'continent', 'tld', 'currency_code', 'currency_name',\n",
    "              'phone', 'postal_code_format', 'postal_code_regex',\n",
    "              'languages', 'neighbours')\n",
    "    if re.search('/view/', url):\n",
    "        tree = fromstring(html)\n",
    "        all_rows = [\n",
    "            tree.xpath('//tr[@id=\"places_%s__row\"]/td[@class=\"w2p_fw\"]' % field)[0].text_content()\n",
    "            for field in fields]\n",
    "        print(url, all_rows)\n",
    "\n",
    "\n",
    "def link_crawler(start_url, link_regex, robots_url=None, user_agent='wswp',\n",
    "                 proxy=None, delay=3, max_depth=4, scrape_callback=None):\n",
    "    \"\"\" Crawl from the given start URL following links matched by link_regex. In the current\n",
    "        implementation, we do not actually scrapy any information.\n",
    "        args:\n",
    "            start_url (str): web site to start crawl\n",
    "            link_regex (str): regex to match for links\n",
    "        kwargs:\n",
    "            robots_url (str): url of the site's robots.txt (default: start_url + /robots.txt)\n",
    "            user_agent (str): user agent (default: wswp)\n",
    "            proxy (str): proxy url, ex 'http://IP' (default: None)\n",
    "            delay (int): seconds to throttle between requests to one domain (default: 3)\n",
    "            max_depth (int): maximum crawl depth (to avoid traps) (default: 4)\n",
    "            scrape_callback (function): function to call after each download (default: None)\n",
    "    \"\"\"\n",
    "    crawl_queue = [start_url]\n",
    "    # keep track which URL's have seen before\n",
    "    seen = {}\n",
    "    data = []\n",
    "    if not robots_url:\n",
    "        robots_url = '{}/robots.txt'.format(start_url)\n",
    "    rp = get_robots_parser(robots_url)\n",
    "    throttle = Throttle(delay)\n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "        # check url passes robots.txt restrictions\n",
    "        if rp.can_fetch(user_agent, url):\n",
    "            depth = seen.get(url, 0)\n",
    "            if depth == max_depth:\n",
    "                print('Skipping %s due to depth' % url)\n",
    "                continue\n",
    "            throttle.wait(url)\n",
    "            html = download(url, user_agent=user_agent, proxy=proxy)\n",
    "            if not html:\n",
    "                continue\n",
    "            if scrape_callback:\n",
    "                data.extend(scrape_callback(url, html) or [])\n",
    "            # filter for links matching our regular expression\n",
    "            for link in get_links(html):\n",
    "                if re.match(link_regex, link):\n",
    "                    abs_link = urljoin(start_url, link)\n",
    "                    if abs_link not in seen:\n",
    "                        seen[abs_link] = depth + 1\n",
    "                        crawl_queue.append(abs_link)\n",
    "        else:\n",
    "            print('Blocked by robots.txt:', url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
